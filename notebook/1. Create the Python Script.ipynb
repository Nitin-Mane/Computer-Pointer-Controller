{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This notebook is for generating python scripts from the notebook\n",
    "\n",
    "Extracting the script files to process the operation of the Computer Pointer Controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u45445/My-Notebooks/Computer-Pointer-Controller/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u45445/My-Notebooks/Computer-Pointer-Controller\n"
     ]
    }
   ],
   "source": [
    "cd /home/u45445/My-Notebooks/Computer-Pointer-Controller/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bin\t   media\t\t models     README.md\t       results\t utils\r\n",
      " main.py  'model install .txt'\t notebook   requirements.txt   src\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting image==1.5.27\n",
      "  Using cached https://files.pythonhosted.org/packages/0c/ec/51969468a8b87f631cc0e60a6bf1e5f6eec8ef3fd2ee45dc760d5a93b82a/image-1.5.27-py2.py3-none-any.whl\n",
      "Collecting django (from image==1.5.27)\n",
      "  Using cached https://files.pythonhosted.org/packages/49/49/178daa8725d29c475216259eb19e90b2aa0b8c0431af8c7e9b490ae6481d/Django-1.11.29-py2.py3-none-any.whl\n",
      "Collecting pillow (from image==1.5.27)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/ad/61f8dfba88c4e56196bf6d056cdbba64dc9c5dfdfbc97d02e6472feed913/Pillow-6.2.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pytz (from django->image==1.5.27)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Installing collected packages: pytz, django, pillow, image\n",
      "Successfully installed django-1.11.29 image-1.5.27 pillow-6.2.2 pytz-2020.1\n",
      "Collecting ipdb==0.12.3\n",
      "Collecting ipython<6.0.0,>=5.1.0; python_version == \"2.7\" (from ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/ce/2c/2849a2b37024a01a847c87d81825c0489eb22ffc6416cac009bf281ea838/ipython-5.10.0-py2-none-any.whl\n",
      "Collecting setuptools (from ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/b7/182161210a13158cd3ccc41ee19aadef54496b74f2817cc147006ec932b4/setuptools-44.1.1-py2.py3-none-any.whl\n",
      "Collecting simplegeneric>0.8 (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "Collecting pickleshare (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/41/220f49aaea88bc6fa6cba8d05ecf24676326156c23b991e80b3f2fc24c77/pickleshare-0.7.5-py2.py3-none-any.whl\n",
      "Collecting pygments<2.6 (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/be/39/32da3184734730c0e4d3fa3b2b5872104668ad6dc1b5a73d8e477e5fe967/Pygments-2.5.2-py2.py3-none-any.whl\n",
      "Collecting backports.shutil-get-terminal-size; python_version == \"2.7\" (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/7d/cd/1750d6c35fe86d35f8562091737907f234b78fdffab42b29c72b1dd861f4/backports.shutil_get_terminal_size-1.0.0-py2.py3-none-any.whl\n",
      "Collecting pathlib2; python_version == \"2.7\" or python_version == \"3.3\" (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\n",
      "Collecting pexpect; sys_platform != \"win32\" (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/7b/88dbb785881c28a102619d46423cb853b46dbccc70d3ac362d99773a78ce/pexpect-4.8.0-py2.py3-none-any.whl\n",
      "Collecting traitlets>=4.2 (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/ca/ab/872a23e29cec3cf2594af7e857f18b687ad21039c1f9b922fac5b9b142d5/traitlets-4.3.3-py2.py3-none-any.whl\n",
      "Collecting prompt-toolkit<2.0.0,>=1.0.4 (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/9d/d2/2f099b5cd62dab819ce7a9f1431c09a9032fbfbb6474f442722e88935376/prompt_toolkit-1.0.18-py2-none-any.whl\n",
      "Collecting decorator (from ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting six (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Collecting scandir; python_version < \"3.5\" (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "Collecting ptyprocess>=0.5 (from pexpect; sys_platform != \"win32\"->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/29/605c2cc68a9992d18dada28206eeada56ea4bd07a239669da41674648b6f/ptyprocess-0.6.0-py2.py3-none-any.whl\n",
      "Collecting enum34; python_version == \"2.7\" (from traitlets>=4.2->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/6f/2c/a9386903ece2ea85e9807e0e062174dc26fdce8b05f216d00491be29fad5/enum34-1.1.10-py2-none-any.whl\n",
      "Collecting ipython-genutils (from traitlets>=4.2->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/fa/bc/9bd3b5c2b4774d5f33b2d544f1460be9df7df2fe42f352135381c347c69a/ipython_genutils-0.2.0-py2.py3-none-any.whl\n",
      "Collecting wcwidth (from prompt-toolkit<2.0.0,>=1.0.4->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/59/7c/e39aca596badaf1b78e8f547c807b04dae603a433d3e7a7e04d67f2ef3e5/wcwidth-0.2.5-py2.py3-none-any.whl\n",
      "Collecting backports.functools-lru-cache>=1.2.1; python_version < \"3.2\" (from wcwidth->prompt-toolkit<2.0.0,>=1.0.4->ipython<6.0.0,>=5.1.0; python_version == \"2.7\"->ipdb==0.12.3)\n",
      "  Using cached https://files.pythonhosted.org/packages/da/d1/080d2bb13773803648281a49e3918f65b31b7beebf009887a529357fd44a/backports.functools_lru_cache-1.6.1-py2.py3-none-any.whl\n",
      "Installing collected packages: simplegeneric, six, scandir, pathlib2, pickleshare, pygments, backports.shutil-get-terminal-size, ptyprocess, pexpect, enum34, ipython-genutils, decorator, traitlets, setuptools, backports.functools-lru-cache, wcwidth, prompt-toolkit, ipython, ipdb\n",
      "Successfully installed backports.functools-lru-cache-1.6.1 backports.shutil-get-terminal-size-1.0.0 decorator-4.4.2 enum34-1.1.10 ipdb-0.12.3 ipython-5.10.0 ipython-genutils-0.2.0 pathlib2-2.3.5 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-1.0.18 ptyprocess-0.6.0 pygments-2.5.2 scandir-1.10.0 setuptools-44.1.1 simplegeneric-0.8.1 six-1.15.0 traitlets-4.3.3 wcwidth-0.2.5\n",
      "Collecting pathlib-1.0.1\n",
      "\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 353, in run\n",
      "    wb.build(autobuilding=True)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/wheel.py\", line 749, in build\n",
      "    self.requirement_set.prepare_files(self.finder)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 380, in prepare_files\n",
      "    ignore_dependencies=self.ignore_dependencies))\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_set.py\", line 554, in _prepare_file\n",
      "    require_hashes\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/req/req_install.py\", line 278, in populate_link\n",
      "    self.link = finder.find_requirement(self, upgrade)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 465, in find_requirement\n",
      "    all_candidates = self.find_all_candidates(req.name)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 423, in find_all_candidates\n",
      "    for page in self._get_pages(url_locations, project_name):\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 568, in _get_pages\n",
      "    page = self._get_page(location)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 683, in _get_page\n",
      "    return HTMLPage.get_page(link, session=self.session)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/index.py\", line 795, in get_page\n",
      "    resp.raise_for_status()\n",
      "  File \"/usr/share/python-wheels/requests-2.18.4-py2.py3-none-any.whl/requests/models.py\", line 935, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "HTTPError: 404 Client Error: Not Found for url: https://pypi.org/simple/pathlib-1-0-1/\u001b[0m\n",
      "Collecting ipython==7.10.2\n",
      "\u001b[31m  Could not find a version that satisfies the requirement ipython==7.10.2 (from versions: 0.10, 0.10.1, 0.10.2, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.13.2, 1.0.0, 1.1.0, 1.2.0, 1.2.1, 2.0.0, 2.1.0, 2.2.0, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 3.0.0, 3.1.0, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 4.0.0b1, 4.0.0, 4.0.1, 4.0.2, 4.0.3, 4.1.0rc1, 4.1.0rc2, 4.1.0, 4.1.1, 4.1.2, 4.2.0, 4.2.1, 5.0.0b1, 5.0.0b2, 5.0.0b3, 5.0.0b4, 5.0.0rc1, 5.0.0, 5.1.0, 5.2.0, 5.2.1, 5.2.2, 5.3.0, 5.4.0, 5.4.1, 5.5.0, 5.6.0, 5.7.0, 5.8.0, 5.9.0, 5.10.0)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for ipython==7.10.2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.17.4\n",
      "\u001b[31m  Could not find a version that satisfies the requirement numpy==1.17.4 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.3, 1.10.4, 1.11.0b3, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.11.1rc1, 1.11.1, 1.11.2rc1, 1.11.2, 1.11.3, 1.12.0b1, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.1rc1, 1.12.1, 1.13.0rc1, 1.13.0rc2, 1.13.0, 1.13.1, 1.13.3, 1.14.0rc1, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for numpy==1.17.4\u001b[0m\n",
      "Collecting PyScreeze==0.1.26\n",
      "Collecting Pillow>=2.0.0; python_version == \"2.7\" (from PyScreeze==0.1.26)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/ad/61f8dfba88c4e56196bf6d056cdbba64dc9c5dfdfbc97d02e6472feed913/Pillow-6.2.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: Pillow, PyScreeze\n",
      "Successfully installed Pillow-6.2.2 PyScreeze-0.1.26\n",
      "Collecting PyTweening==1.0.3\n",
      "Installing collected packages: PyTweening\n",
      "Successfully installed PyTweening-1.0.3\n",
      "Collecting PyMsgBox==1.0.7\n",
      "Installing collected packages: PyMsgBox\n",
      "Successfully installed PyMsgBox-1.0.8\n",
      "Collecting Pillow==6.2.1\n",
      "  Using cached https://files.pythonhosted.org/packages/1b/08/ff620ef5a6128ee6e7a505f5716f81fce7b71f3a69e99646ebe64e0b9984/Pillow-6.2.1-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-6.2.2\n",
      "Collecting requests==2.22.0\n",
      "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests==2.22.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests==2.22.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests==2.22.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests==2.22.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Installing collected packages: urllib3, certifi, chardet, idna, requests\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 idna-2.8 requests-2.22.0 urllib3-1.25.10\n",
      "Collecting virtualenv==16.7.9\n",
      "  Using cached https://files.pythonhosted.org/packages/05/f1/2e07e8ca50e047b9cc9ad56cf4291f4e041fa73207d000a095fe478abf84/virtualenv-16.7.9-py2.py3-none-any.whl\n",
      "Installing collected packages: virtualenv\n",
      "Successfully installed virtualenv-16.7.9\n",
      "Collecting pyautogui==0.9.50\n",
      "Collecting mouseinfo (from pyautogui==0.9.50)\n",
      "Collecting pyscreeze>=0.1.21 (from pyautogui==0.9.50)\n",
      "Collecting pygetwindow>=0.0.5 (from pyautogui==0.9.50)\n",
      "Collecting PyTweening>=1.0.1 (from pyautogui==0.9.50)\n",
      "Collecting python-xlib; platform_system == \"Linux\" and python_version < \"3.0\" (from pyautogui==0.9.50)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/10/2eb938852a9bdf6745808f141c9fede76b1bd5a9530859bacc71985d29d9/python_xlib-0.27-py2.py3-none-any.whl\n",
      "Collecting pymsgbox (from pyautogui==0.9.50)\n",
      "Collecting Xlib; platform_system == \"Linux\" and python_version < \"3.0\" (from mouseinfo->pyautogui==0.9.50)\n",
      "  Using cached https://files.pythonhosted.org/packages/3f/00/321541273b0ed2167b36c82be9baeb0bdc8af1c11c1b01de9436b84b5eaf/xlib-0.21-py2.py3-none-any.whl\n",
      "Collecting Pillow>=2.0.0; python_version == \"2.7\" (from mouseinfo->pyautogui==0.9.50)\n",
      "  Using cached https://files.pythonhosted.org/packages/12/ad/61f8dfba88c4e56196bf6d056cdbba64dc9c5dfdfbc97d02e6472feed913/Pillow-6.2.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pyperclip (from mouseinfo->pyautogui==0.9.50)\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui==0.9.50)\n",
      "Collecting six>=1.10.0 (from python-xlib; platform_system == \"Linux\" and python_version < \"3.0\"->pyautogui==0.9.50)\n",
      "  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, Xlib, Pillow, pyperclip, mouseinfo, pyscreeze, pyrect, pygetwindow, PyTweening, python-xlib, pymsgbox, pyautogui\n",
      "Successfully installed Pillow-6.2.2 PyTweening-1.0.3 Xlib-0.21 mouseinfo-0.1.3 pyautogui-0.9.50 pygetwindow-0.0.8 pymsgbox-1.0.8 pyperclip-1.8.0 pyrect-0.1.4 pyscreeze-0.1.26 python-xlib-0.27 six-1.15.0\n",
      "Collecting pyobjc-core==6.2\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pyobjc-core==6.2 (from versions: 2.2b1, 2.2b2, 2.2, 2.3, 2.4, 2.5.1, 3.0.1, 3.0.3, 3.0.4, 3.1, 3.1.1, 3.2, 3.2.1b1, 3.2.1, 4.0b1, 4.0, 4.0.1, 4.1, 4.2, 4.2.1, 4.2.2, 5.0a1, 5.0b1, 5.0, 5.1, 5.1.1, 5.1.2, 5.2, 5.3)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pyobjc-core==6.2\u001b[0m\n",
      "Collecting pyobjc-framework-Cocoa==6.1\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pyobjc-framework-Cocoa==6.1 (from versions: 2.2b1, 2.2b2, 2.2, 2.3, 2.4, 2.5.1, 3.0.1, 3.0.3, 3.0.4, 3.1, 3.1.1, 3.2, 3.2.1b1, 3.2.1, 4.0b1, 4.0, 4.0.1, 4.1, 4.2, 4.2.1, 4.2.2, 5.0a1, 5.0b1, 5.0, 5.1, 5.1.1, 5.1.2, 5.2, 5.3)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pyobjc-framework-Cocoa==6.1\u001b[0m\n",
      "Collecting pyobjc-framework-Quartz==5.3\n",
      "  Using cached https://files.pythonhosted.org/packages/ce/e3/be5dd50240fe90d597c2afabc3cd27304ba5bfc66354f5fa52b0fa5bfe09/pyobjc-framework-Quartz-5.3.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-shTMJn/pyobjc-framework-Quartz/setup.py\", line 61, in <module>\n",
      "        [ 'Modules/_CVPixelBuffer.m' ]),\n",
      "      File \"pyobjc_setup.py\", line 392, in Extension\n",
      "        os_level = get_os_level()\n",
      "      File \"pyobjc_setup.py\", line 206, in get_os_level\n",
      "        pl = plistlib.readPlist('/System/Library/CoreServices/SystemVersion.plist')\n",
      "      File \"/usr/lib/python2.7/plistlib.py\", line 75, in readPlist\n",
      "        pathOrFile = open(pathOrFile)\n",
      "    IOError: [Errno 2] No such file or directory: '/System/Library/CoreServices/SystemVersion.plist'\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-shTMJn/pyobjc-framework-Quartz/\u001b[0m\n",
      "Collecting opencv-python==4.1.0.25\n",
      "  Using cached https://files.pythonhosted.org/packages/77/30/36c3f0644fa9f42d92f079b972e990a5874c1fc2b2c0e9656eb88bb8d6dc/opencv_python-4.1.0.25-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.11.1 (from opencv-python==4.1.0.25)\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-1.16.6 opencv-python-4.1.0.25\n",
      "Collecting ffmpeg-python==0.2.0\n",
      "Collecting future (from ffmpeg-python==0.2.0)\n",
      "Installing collected packages: future, ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0 future-0.18.2\n",
      "Collecting olefile==0.46\n",
      "Installing collected packages: olefile\n",
      "Successfully installed olefile-0.46\n"
     ]
    }
   ],
   "source": [
    "! pip install image==1.5.27\n",
    "! pip install ipdb==0.12.3\n",
    "! pip install pathlib-1.0.1\n",
    "! pip install ipython==7.10.2\n",
    "! pip install numpy==1.17.4\n",
    "! pip install PyScreeze==0.1.26\n",
    "! pip install PyTweening==1.0.3\n",
    "! pip install PyMsgBox==1.0.7\n",
    "! pip install Pillow==6.2.1\n",
    "! pip install requests==2.22.0\n",
    "! pip install virtualenv==16.7.9\n",
    "! pip install pyautogui==0.9.50\n",
    "! pip install pyobjc-core==6.2\n",
    "! pip install pyobjc-framework-Cocoa==6.1\n",
    "! pip install pyobjc-framework-Quartz==5.3\n",
    "! pip install opencv-python==4.1.0.25\n",
    "! pip install ffmpeg-python==0.2.0\n",
    "! pip install olefile==0.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u45445/My-Notebooks/Computer-Pointer-Controller/notebook\n"
     ]
    }
   ],
   "source": [
    "cd /home/u45445/My-Notebooks/Computer-Pointer-Controller/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u45445/My-Notebooks/Computer-Pointer-Controller/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting face_detection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile face_detection.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#########################################################################################################\n",
    "#\n",
    "#                                    Face Detection Script\n",
    "#\n",
    "#########################################################################################################\n",
    "'''\n",
    "Face Detection Script\n",
    "\n",
    "This library helps for face dectection \n",
    "\n",
    "This is a sample class for a model. You may choose to use it as-is or make any changes to it.\n",
    "Since you will be using four models to build this project, you will need to replicate this file\n",
    "for each of the models.\n",
    "This has been provided just to give you an idea of how to structure your model class.\n",
    "'''\n",
    "\n",
    "# Load the system libraries \n",
    "import numpy as np \n",
    "from numpy import clip\n",
    "from ie_module import Module\n",
    "from helper import resize_input\n",
    "\n",
    "'''\n",
    "Face Dectector Class: \n",
    "\n",
    "This class define the class for the face detection library. \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "class Face_Detection(Module):\n",
    "    '''\n",
    "    This is the face detection class module\n",
    "    It require the \n",
    "    the face input parameter are process for the finding the position and evaluating the outcome source file\n",
    "\n",
    "    '''\n",
    "    class Result:\n",
    "        \n",
    "        OUTPUT_SIZE_NUM = 7\n",
    "\n",
    "        def __init__(self, outputfd):\n",
    "            '''\n",
    "            Initializing the face dectector function\n",
    "            this will help to pass the image ID, labels, confidence range and the postion of the frame. \n",
    "            '''\n",
    "            # taking the image id\n",
    "            self.image_id = outputfd[0]\n",
    "            # creating the label\n",
    "            self.label = int(outputfd[1])\n",
    "            # creating the confidence\n",
    "            self.confidence = outputfd[2]\n",
    "            # creating the position array\n",
    "            self.position = np.array((outputfd[3], outputfd[4])) # (x, y)\n",
    "            # resize the array outcome\n",
    "            self.size = np.array((outputfd[5], outputfd[6])) # (w, h)\n",
    "\n",
    "        def rescale_roi(self, roi_scale_factor=1.0):\n",
    "            '''\n",
    "            Rescale ROI: Specify a position constraint function inside the boundary of the image size.\n",
    "            '''\n",
    "            # position to set\n",
    "            self.position -= self.size * 0.5 * (roi_scale_factor - 1.0)\n",
    "            # check the ROI scale factor size\n",
    "            self.size *= roi_scale_factor\n",
    "\n",
    "        def resize_roi(self, frame_width, frame_height):\n",
    "            '''\n",
    "            Resize the ROI: Enable resizing of ROI object, specified as positon or size.\n",
    "            '''\n",
    "            # position outcome\n",
    "            self.position[0] *= frame_width\n",
    "            self.position[1] *= frame_height\n",
    "            self.size[0] = self.size[0] * frame_width - self.position[0]\n",
    "            self.size[1] = self.size[1] * frame_height - self.position[1]\n",
    "\n",
    "        def clip(self, width, height):\n",
    "            '''\n",
    "            Clip: Create the clip frames size of the min and max frame with position and size\n",
    "            '''\n",
    "            min = [0, 0]\n",
    "            max = [width, height]\n",
    "            self.position[:] = clip(self.position, min, max)\n",
    "            self.size[:] = clip(self.size, min, max)\n",
    "            \n",
    "\n",
    "    def __init__(self, model, confidence_threshold=0.5, roi_scale_factor=1.15):\n",
    "        '''\n",
    "        Initilizing the model with the ROI scale factor\n",
    "\n",
    "        Input: \n",
    "               Model \n",
    "\n",
    "        Outcome: \n",
    "                Shape\n",
    "        '''\n",
    "        super(Face_Detection, self).__init__(model)\n",
    "        # finding the length of the model input and output\n",
    "        assert len(model.inputs) == 1, \"Expected 1 input blob\"\n",
    "        assert len(model.outputs) == 1, \"Expected 1 output blob\"\n",
    "        # Set the blob and the shape with respect to the model\n",
    "        self.input_blob = next(iter(model.inputs))\n",
    "        self.output_blob = next(iter(model.outputs))\n",
    "        self.input_shape = model.inputs[self.input_blob].shape\n",
    "        self.output_shape = model.outputs[self.output_blob].shape\n",
    "        # Set the outcome with the shape of the model size\n",
    "        assert len(self.output_shape) == 4 and \\\n",
    "               self.output_shape[3] == self.Result.OUTPUT_SIZE_NUM, \\\n",
    "            \"Expected model output shape with %s outputs\" % \\\n",
    "            (self.Result.OUTPUT_SIZE_NUM)\n",
    "        # if the confidence threshold set to the 0.0 to 1.0 inbetween\n",
    "        assert 0.0 <= confidence_threshold and confidence_threshold <= 1.0, \\\n",
    "            \"Confidence threshold is expected to be in range [0; 1]\"\n",
    "        #checkthe threshold range\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "        assert 0.0 < roi_scale_factor, \"Expected positive ROI scale factor\"\n",
    "        # set the ROI scale factor\n",
    "        self.roi_scale_factor = roi_scale_factor\n",
    "\n",
    "    def preprocess(self, frame):\n",
    "        # Image pre-processing technique for the reshaping the frame\n",
    "        # Note - this is essential for the resizing the irregular and large video frame to fit model input\n",
    "        assert len(frame.shape) == 4, \"Frame shape should be in format as [1, c, h, w]\"\n",
    "        # Set the frame shape in the batch and c\n",
    "        assert frame.shape[0] == 1 \n",
    "        assert frame.shape[1] == 3\n",
    "        # Resize input frame\n",
    "        input = resize_input(frame, self.input_shape)\n",
    "\n",
    "        return (input)\n",
    "\n",
    "    def enqueue(self, input):\n",
    "        #  insert function using list and super\n",
    "        \n",
    "        return (super(Face_Detection, self).enqueue({self.input_blob: input}))\n",
    "\n",
    "    def start_async(self, frame):\n",
    "        # Async technique for the pre-process frame\n",
    "        # The async function is a coroutine return statements or the information which are essential\n",
    "        input = self.preprocess(frame)\n",
    "        # enqueue the input process\n",
    "        self.enqueue(input)\n",
    "\n",
    "\n",
    "    def get_roi_proposals(self, frame):\n",
    "        outputs = self.get_outputs()[0][self.output_blob]\n",
    "        # outputs shape is [N_requests, 1, 1, N_max_faces, 7]\n",
    "        # set the frame width\n",
    "        frame_width = frame.shape[-1]\n",
    "        # set the frame height\n",
    "        frame_height = frame.shape[-2]\n",
    "        # process the result\n",
    "        # set the result empty\n",
    "        results = []\n",
    "        # output progress in the result box format\n",
    "        for outputfd in outputs[0][0]:\n",
    "            # output model detection stage\n",
    "            result = Face_Detection.Result(outputfd)\n",
    "            # conditional statements of the confidence\n",
    "            if result.confidence < self.confidence_threshold:\n",
    "                break \n",
    "            # results are sorted by confidence decrease\n",
    "\n",
    "            # resize the width results\n",
    "            result.resize_roi(frame_width, frame_height)\n",
    "            # rescale the ROI\n",
    "            result.rescale_roi(self.roi_scale_factor)\n",
    "            # Clip the frame width and height\n",
    "            result.clip(frame_width, frame_height)\n",
    "            # create the list format\n",
    "            results.append(result)\n",
    "\n",
    "        return (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Positon Estimation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting head_postion_estimation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile head_postion_estimation.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "##########################################################################################################\n",
    "#\n",
    "#                                Head Positon Estimation Script\n",
    "#\n",
    "#########################################################################################################\n",
    "'''\n",
    "Head Position Estimation Script \n",
    "\n",
    "Head Pose Estimation research is focuses on the prediction of the pose of a human head in an image. \n",
    "More specifically it concerns the prediction of the Euler angles of a human head. \n",
    "The Euler angles consists of three values: yaw, pitch and roll.\n",
    "(Doc)[https://towardsdatascience.com/head-pose-estimation-with-hopenet-5e62ace254d5#:~:text=As%20the%20name%20suggests%2C%20Head,%3A%20yaw%2C%20pitch%20and%20roll.]\n",
    "'''\n",
    "# Load libraries\n",
    "import os # load the operating system library\n",
    "import sys # load the system library\n",
    "# load technical library\n",
    "import cv2 # load the OpenCV library\n",
    "from math import cos, sin, pi # load the math library for configuration\n",
    "# load the log and arguments parser library\n",
    "import logging as log\n",
    "import argparse \n",
    "# load the external libray for pre-processing and model pipeline\n",
    "from ie_module import Module\n",
    "from helper import cut_rois, resize_input\n",
    "\n",
    "class Head_Pose_Estimator(Module):\n",
    "    '''\n",
    "    Head Pose Estimator Class: \n",
    "    This help to detect the head position direction and movement of the person action.\n",
    "    It provide a face dots containing a human face. Then the face process is expanded and transformed to a dots to suit the needs of later steps.\n",
    "    '''\n",
    "    class Result:\n",
    "        \n",
    "        def __init__(self,output):\n",
    "            '''\n",
    "            Initializing the head position in x,y,z-axis\n",
    "            Interactive 3D Graphics\n",
    "            [Info](https://www.youtube.com/watch?v=q0jgqeS_ACM&feature=youtu.be) \n",
    "            '''\n",
    "            self.head_position_x = output[\"angle_y_fc\"][0] #Yaw\n",
    "            self.head_position_y = output[\"angle_p_fc\"][0] #Pitch\n",
    "            self.head_position_z = output[\"angle_r_fc\"][0] #Roll\n",
    "\n",
    "    def __init__(self, model):\n",
    "        # initalizing the class\n",
    "        super(Head_Pose_Estimator, self).__init__(model)\n",
    "        \n",
    "        # input 1 blob to the model length\n",
    "        assert len(model.inputs) == 1, \"Expected 1 input blob process\"\n",
    "        \n",
    "        # output at the 3 blob\n",
    "        assert len(model.outputs) == 3, \"Expected 1 output blob process\"\n",
    "        \n",
    "        # Process the blob data\n",
    "        self.input_blob = next(iter(model.inputs)) # model input\n",
    "        self.output_blob = next(iter(model.outputs)) # output blob\n",
    "        \n",
    "        # model resize formation\n",
    "        self.input_shape = model.inputs[self.input_blob].shape\n",
    "\n",
    "    def preprocess(self, frame, rois):\n",
    "        # pre-processing technique\n",
    "        # the frame shape are reshaped with the model input parameter\n",
    "        assert len(frame.shape) == 4, \"Frame shape should be [1, c, h, w]\"\n",
    "        \n",
    "        inputs = cut_rois(frame, rois)\n",
    "        inputs = [resize_input(input, self.input_shape) for input in inputs]\n",
    "        \n",
    "        # the input frames are shape and resize\n",
    "        return (inputs)\n",
    "    \n",
    "    def enqueue(self, input):\n",
    "        # enqueue: the head positions estimating the results in queue\n",
    "        \n",
    "        return super(Head_Pose_Estimator, self).enqueue({self.input_blob: input})\n",
    "    \n",
    "    def start_async(self, frame, rois):\n",
    "        # Starting the async techniques for the frame\n",
    "        \n",
    "        inputs = self.preprocess(frame, rois)\n",
    "        \n",
    "        for input in inputs:\n",
    "            # outcome in queue for the input frame for further process\n",
    "            self.enqueue(input)\n",
    "\n",
    "    def get_headposition(self):\n",
    "        # Head position of the output\n",
    "        \n",
    "        outputs = self.get_outputs()\n",
    "        \n",
    "        # processing the head position results\n",
    "        return (Head_Pose_Estimator.Result(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmark Detection Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting landmark_detection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile landmark_detection.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#####################################################################################################\n",
    "#\n",
    "#                                      Landmark Detection Script\n",
    "#\n",
    "######################################################################################################\n",
    "\n",
    "'''\n",
    "Landmark Detection Script: \n",
    "Facial landmarks are used to localize and represent salient regions of the face\n",
    "This are following \n",
    "1. Eyes\n",
    "2. Nose \n",
    "3. Mouth \n",
    "\n",
    "Detecting facial landmark frames by frames into a two step process:\n",
    "\n",
    "Step #1: Localize the face in the image frame and preprocessing.\n",
    "Step #2: Detect the key facial structures on the face ROI frames.\n",
    "\n",
    "[docs](https://medium.com/analytics-vidhya/facial-landmarks-and-face-detection-in-python-with-opencv-73979391f30e)\n",
    "'''\n",
    "# load the library\n",
    "import numpy as np # load numerical operation library\n",
    "# load external libary for model pipeline and pre-processing frames.\n",
    "from helper import cut_rois, resize_input\n",
    "from ie_module import Module\n",
    "\n",
    "class Landmarks_Detection(Module):\n",
    "    '''\n",
    "    Land Marks Detection Class\n",
    "    this class represent the processing of the land mark operation.\n",
    "    It will generate the land mark map of the face and it can be process for the detection process. \n",
    "\n",
    "    the main process is to pre-process the frames images and extract the ROI regions and add facial structures nodes points.\n",
    "    This is will happen with help of the following inputs \n",
    "    1. Left Eye\n",
    "    2. Right Eye\n",
    "    3. Nose Tip \n",
    "    4. Left lips corner\n",
    "    5. Right lips corner \n",
    "\n",
    "    the points are mapped based on the region detected and passsing the nearby area pixels\n",
    "    '''\n",
    "    POINTS_NUMBER = 5\n",
    "\n",
    "    class Result:\n",
    "        # Result class \n",
    "        \n",
    "        # Initialize the class\n",
    "        def __init__(self, outputs):\n",
    "            \n",
    "            # output set\n",
    "            self.points = outputs\n",
    "            \n",
    "            #lambda: A lambda function is a small anonymous function\n",
    "            lm = lambda i: self[i]\n",
    "            \n",
    "            # left eyes pointer\n",
    "            self.left_eye = lm(0)\n",
    "            \n",
    "            # right eyes pointer\n",
    "            self.right_eye = lm(1)\n",
    "            \n",
    "            # nose tip pointer\n",
    "            self.nose_tip = lm(2)\n",
    "            \n",
    "            # left lips corner pointer\n",
    "            self.left_lip_corner = lm(3)\n",
    "            \n",
    "            # right lip corner pointer\n",
    "            self.right_lip_corner = lm(4)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            \n",
    "            # Get the item list index\n",
    "            return self.points[idx]\n",
    "\n",
    "        def get_array(self):\n",
    "            \n",
    "            # get the array points\n",
    "            return np.array(self.points, dtype=np.float64)\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        # Initialize the landmark model\n",
    "        super(Landmarks_Detection, self).__init__(model)\n",
    "\n",
    "        self.update = False\n",
    "        \n",
    "        # update the input and out\n",
    "        assert len(model.inputs) == 1, \"Expected 1 input blob\"\n",
    "        assert len(model.outputs) == 1, \"Expected 1 output blob\"\n",
    "        \n",
    "        # blob process\n",
    "        self.input_blob = next(iter(model.inputs))\n",
    "        self.output_blob = next(iter(model.outputs))\n",
    "        self.input_shape = model.inputs[self.input_blob].shape\n",
    "\n",
    "    def preprocess(self, frame, rois):\n",
    "        \n",
    "        # pre-processing technique\n",
    "        assert len(frame.shape) == 4, \"Frame shape should be [1, c, h, w]\"\n",
    "        \n",
    "        # input provides the frame with ROI region\n",
    "        inputs = cut_rois(frame, rois)\n",
    "        \n",
    "        # resizing the frame and processing the shape input it\n",
    "        inputs = [resize_input(input, self.input_shape) for input in inputs]\n",
    "        return inputs\n",
    "\n",
    "    def enqueue(self, input):\n",
    "        \n",
    "        # enqueue the landmark detector modules\n",
    "        return super(Landmarks_Detection, self).enqueue({self.input_blob: input})\n",
    "\n",
    "    def start_async(self, frame, rois):\n",
    "        \n",
    "        # Starting the async technique process\n",
    "        inputs = self.preprocess(frame, rois)\n",
    "        \n",
    "        # When the input are process it get in queue\n",
    "        for input in inputs:\n",
    "            self.enqueue(input)\n",
    "\n",
    "    def get_landmarks(self):\n",
    "        \n",
    "        # Outcome of landmarks\n",
    "        outputs = self.get_outputs()\n",
    "        \n",
    "        # output processed\n",
    "        results = [Landmarks_Detection.Result(out[self.output_blob].reshape((-1, 2))) \\\n",
    "                      for out in outputs]\n",
    "        \n",
    "        # Outcome\n",
    "        return (results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze Estimator Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gaze_Estimator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gaze_Estimator.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#############################################################################################################\n",
    "#\n",
    "#                                  Gaze Estimator Script\n",
    "#\n",
    "#############################################################################################################\n",
    "\n",
    "'''\n",
    "Gaze Estimator Script \n",
    "\n",
    "Provides script to perform offline gaze estimation from eyetracking video frame and realtime operation. \n",
    "There are eye tracking classes and estimating the direction flow\n",
    "\n",
    "'''\n",
    "\n",
    "# Load system libraries\n",
    "import os # load operating system library\n",
    "import sys # load system library\n",
    "import logging as log # load the logs library\n",
    "# Load processing libraries\n",
    "import numpy as np # load numerical analysis library\n",
    "import cv2 # Load image processing library\n",
    "# Load OpenVINO library\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "from ie_module import Module\n",
    "\n",
    "class Gaze_Estimation(Module):\n",
    "    \"\"\"\n",
    "    Gaze Estimator Class:\n",
    "     For gaze estimation model it require three input blobs as follows\n",
    "         1. right_eye_image\n",
    "         2. head_pose_angles\n",
    "         3. left_eye_image\n",
    "    Input: \n",
    "          Load and configure inference plugins for the specified target devices and performs synchronous and asynchronous modes for the specified infer requests.\n",
    "    Output: \n",
    "          Direction flow vectors\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        # Initilization gaze estimation class\n",
    "        # Setup inital stage\n",
    "        super(Gaze_Estimation, self).__init__(model)\n",
    "        \n",
    "        assert len(model.inputs) == 3, \"Expected 1 input blob\"\n",
    "        assert len(model.outputs) == 1, \"Expected 1 output blob\"\n",
    "        # gaze estimation image input with the shape\n",
    "\n",
    "        self.input_blob = [] \n",
    "        self.input_shape = []\n",
    "        \n",
    "        # Append the input and models shapes\n",
    "        for inputs in model.inputs:\n",
    "            self.input_blob.append(inputs)\n",
    "            self.input_shape.append(model.inputs[inputs].shape)\n",
    "        self.output_blob = next(iter(model.outputs))\n",
    "    \n",
    "    def enqueue(self, head_pose, right_eye, left_eye):\n",
    "        # enqueue the gaze estimator input\n",
    "        \n",
    "        return super(Gaze_Estimation, self).enqueue({'left_eye_image': left_eye,\n",
    "                                                    'right_eye_image': right_eye,\n",
    "                                                    'head_pose_angles': head_pose})\n",
    "\n",
    "    def start_async(self, headPosition, right_eye_image, left_eye_image):\n",
    "        \n",
    "        # Async process start\n",
    "        head_pose = [headPosition.head_position_x, # x-axis\n",
    "                    headPosition.head_position_y,  # y-axis\n",
    "                    headPosition.head_position_z] # z-axis\n",
    "        \n",
    "        # set head post array\n",
    "        head_pose = np.array([head_pose])\n",
    "        head_pose = head_pose.flatten()\n",
    "        \n",
    "        # set the left eye information in the axis scale\n",
    "        left_eye = cv2.resize(left_eye_image, (60, 60), interpolation = cv2.INTER_AREA)\n",
    "        left_eye = np.moveaxis(left_eye, -1, 0)\n",
    "        \n",
    "        # set the right eye information in axis scale\n",
    "        right_eye = cv2.resize(right_eye_image, (60, 60), interpolation = cv2.INTER_AREA)\n",
    "        right_eye = np.moveaxis(right_eye, -1, 0)\n",
    "        \n",
    "        #enqueue the position and eye processed information\n",
    "        self.enqueue(head_pose, right_eye, left_eye)\n",
    "        \n",
    "    def get_gazevector(self):\n",
    "        \n",
    "        # Getting the gaze vector information\n",
    "        outputs = self.get_outputs()\n",
    "        \n",
    "        # the output generated a vectors \n",
    "        return (outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input feeder script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input_feeder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile input_feeder.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "'''\n",
    "This class can be used to feed input from an image, webcam, or video to your model.\n",
    "Sample usage:\n",
    "    feed=InputFeeder(input_type='video', input_file='video.mp4')\n",
    "    feed.load_data()\n",
    "    for batch in feed.next_batch():\n",
    "        do_something(batch)\n",
    "    feed.close()\n",
    "'''\n",
    "import cv2\n",
    "from numpy import ndarray\n",
    "\n",
    "class InputFeeder:\n",
    "    def __init__(self, input_type, input_file=None):\n",
    "        '''\n",
    "        input_type: str, The type of input. Can be 'video' for video file, 'image' for image file,\n",
    "                    or 'cam' to use webcam feed.\n",
    "        input_file: str, The file that contains the input image or video file. Leave empty for cam input_type.\n",
    "        '''\n",
    "        self.input_type=input_type\n",
    "        if input_type=='video' or input_type=='image':\n",
    "            self.input_file=input_file\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.input_type=='video':\n",
    "            self.cap=cv2.VideoCapture(self.input_file)\n",
    "        elif self.input_type=='cam':\n",
    "            self.cap=cv2.VideoCapture(0)\n",
    "        else:\n",
    "            self.cap=cv2.imread(self.input_file)\n",
    "\n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Returns the next image from either a video file or webcam.\n",
    "        If input_type is 'image', then it returns the same image.\n",
    "        '''\n",
    "        while True:\n",
    "            for _ in range(10):\n",
    "                _, frame=self.cap.read()\n",
    "            yield frame\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        '''\n",
    "        Closes the VideoCapture.\n",
    "        '''\n",
    "        if not self.input_type=='image':\n",
    "            self.cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouse Controller Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mouse_controller.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mouse_controller.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "######################################################################################################\n",
    "#\n",
    "#                             Mouse Controller Script\n",
    "#\n",
    "######################################################################################################\n",
    "\n",
    "'''\n",
    "This is a sample class that you can use to control the mouse pointer.\n",
    "It uses the pyautogui library. You can set the precision for mouse movement\n",
    "(how much the mouse moves) and the speed (how fast it moves) by changing \n",
    "precision_dict and speed_dict.\n",
    "Calling the move function with the x and y output of the gaze estimation model\n",
    "will move the pointer.\n",
    "This class is provided to help get you started; you can choose whether you want to use it or create your own from scratch.\n",
    "'''\n",
    "'''\n",
    "import pyautogui\n",
    "\n",
    "class Mouse_Controller_Pointer:\n",
    "    def __init__(self, precision, speed):\n",
    "        precision_dict={'high':100, 'low':1000, 'medium':500}\n",
    "        speed_dict={'fast':1, 'slow':10, 'medium':5}\n",
    "\n",
    "        self.precision=precision_dict[precision]\n",
    "        self.speed=speed_dict[speed]\n",
    "\n",
    "    def move(self, x, y):\n",
    "        pyautogui.moveRel(x*self.precision, -1*y*self.precision, duration=self.speed)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Mouse Process Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mouse_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mouse_process.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "############################################################################################################\n",
    "#\n",
    "#                              Mouse Process Script\n",
    "#\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# load the library\n",
    "# load the system libary\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# load numerical operation library\n",
    "import numpy as np\n",
    "from math import cos, sin, pi \n",
    "\n",
    "# load the log librarys\n",
    "import logging as log\n",
    "\n",
    "# load OpenCV library\n",
    "import cv2\n",
    "\n",
    "# load the Argument Parser for user input\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# load the model and input feeder library (custom)\n",
    "from ie_module import Inference_Context\n",
    "from helper import cut_rois, resize_input\n",
    "from face_detection import Face_Detection\n",
    "from head_position_estimation import Head_Pose_Estimator\n",
    "from landmark_detection import Landmarks_Detection\n",
    "from gaze_Estimator import Gaze_Estimation\n",
    "#from mouse_controller import Mouse_Controller_Pointer\n",
    "from model_feeder import Process_On_Frame\n",
    "\n",
    "class Mouse_Controller:\n",
    "    BREAK_KEY_LABELS = \"q(Q) or Escape\"\n",
    "    BREAK_KEYS = {ord('q'), ord('Q'), 27}\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.frame_processor = Process_On_Frame(args)\n",
    "        self.display = True\n",
    "        self.print_perf_stats = args.perf_stats\n",
    "        # Setup the instruction arguments input\n",
    "        self.fd_out = args.o_fd # Face detection\n",
    "        self.hp_out = args.o_hp # Head position\n",
    "        self.lm_out = args.o_lm # Land mark detection\n",
    "        self.gm_out = args.o_gm # Gaze detection\n",
    "        self.mc_out = args.o_mc # Mouse counter\n",
    "\n",
    "        self.frame_time = 0\n",
    "        self.frame_start_time = 0\n",
    "        self.fps = 0\n",
    "        self.frame_num = 0\n",
    "        self.frame_count = -1\n",
    "        self.right_eye_coords = None\n",
    "        self.left_eye_coords = None \n",
    "        \n",
    "        # Most controller\n",
    "        #self.mc = Mouse_Controller_Pointer('medium','fast')\n",
    "        \n",
    "        self.input_crop = None\n",
    "        if args.crop_width and args.crop_height:\n",
    "            self.input_crop = np.array((args.crop_width, args.crop_height))\n",
    "\n",
    "        self.frame_timeout = 0 if args.timelapse else 1\n",
    "    \n",
    "    \n",
    "    def update_fps(self):\n",
    "        \"\"\"\n",
    "        Calculate FPS time\n",
    "        \"\"\"\n",
    "        now = time.time()\n",
    "        self.frame_time = now - self.frame_start_time\n",
    "        self.fps = 1.0 / self.frame_time\n",
    "        self.frame_start_time = now\n",
    "        return (self.fps)\n",
    "        \n",
    "    \n",
    "    def draw_detection_roi(self, frame, roi):\n",
    "        \"\"\"\n",
    "        Draw Face detection bounding Box\n",
    "\n",
    "        Args:\n",
    "        frame: \n",
    "              The Input Frame\n",
    "        roi: \n",
    "            [xmin, xmax, ymin, ymax]\n",
    "        \"\"\"\n",
    "        for i in range(len(roi)):\n",
    "            # Draw face ROI border\n",
    "            cv2.rectangle(frame,\n",
    "                        tuple(roi[i].position), tuple(roi[i].position + roi[i].size),\n",
    "                        (0, 220, 0), 2)\n",
    "\n",
    "    \n",
    "    def createEyeBoundingBox(self, point1, point2, scale=1.8):\n",
    "        \"\"\"\n",
    "        Create a Eye bounding box using Two points that we got from headposition model\n",
    "\n",
    "        Args:\n",
    "        point1: \n",
    "                First Point coordinate\n",
    "        point2: \n",
    "                Second Point coordinate\n",
    "        \"\"\"\n",
    "\n",
    "        # Normalize the two points\n",
    "        size  = cv2.norm(np.float32(point1) - point2)\n",
    "        width = int(scale * size)\n",
    "        height = width\n",
    "        \n",
    "        # Find x, y mid point\n",
    "        midpoint_x = (point1[0] + point2[0]) / 2\n",
    "        midpoint_y = (point1[1] + point2[1]) / 2\n",
    "\n",
    "        # Calculate eye x, y point\n",
    "        startX = midpoint_x - (width / 2)\n",
    "        startY = midpoint_y - (height / 2)\n",
    "        return [int(startX), int(startY), int(width), int(height)]\n",
    "\n",
    "    \n",
    "    def landmarkPostProcessing(self, frame, landmarks, roi, org_frame):\n",
    "        \"\"\"\n",
    "        Calculate right eye bounding box and left eye bounding box by using\n",
    "        landmark keypoints\n",
    "\n",
    "        Args:\n",
    "        frame: \n",
    "               Frame to resize/crop\n",
    "        landmark: \n",
    "               Keypoints\n",
    "        ROI: \n",
    "               Detection output of Facial detection model\n",
    "        org_frame: \n",
    "               Orginal frame\n",
    "\n",
    "        return:\n",
    "\n",
    "               list of left and right bounding box\n",
    "        \"\"\"\n",
    "        # setup the face bounding box width and height\n",
    "        faceBoundingBoxWidth = roi[0].size[0]\n",
    "        faceBoundingBoxHeight = roi[0].size[1]\n",
    "        # creating the landmark pointers from the cropping the eyes\n",
    "        keypoints = [landmarks.left_eye,\n",
    "                     landmarks.right_eye,\n",
    "                     landmarks.nose_tip,\n",
    "                     landmarks.left_lip_corner,\n",
    "                     landmarks.right_lip_corner]\n",
    "        # Land marks pre-process using computer vision algorithm\n",
    "        '''\n",
    "        [link](https://www.pyimagesearch.com/2017/04/17/real-time-facial-landmark-detection-opencv-python-dlib/)\n",
    "        '''\n",
    "        faceLandmarks = []\n",
    "        # setup the edge detection ROI of the left eye\n",
    "        left_eye_x = (landmarks.left_eye[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        left_eye_y = (landmarks.left_eye[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        # adding the face land marks points x and y axis\n",
    "        faceLandmarks.append([left_eye_x, left_eye_y])\n",
    "        \n",
    "        # setup the edge detection ROI of the right eye\n",
    "        right_eye_x = (landmarks.right_eye[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        right_eye_y = (landmarks.right_eye[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        # adding the face land marks points of x and y axis\n",
    "        faceLandmarks.append([right_eye_x, right_eye_y])\n",
    "        \n",
    "        # setup the corner egde detection ROI of nose tip\n",
    "        nose_tip_x = (landmarks.nose_tip[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        nose_tip_y = (landmarks.nose_tip[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        # adding the face land marks points of x and y axes\n",
    "        faceLandmarks.append([nose_tip_x, nose_tip_y])\n",
    "        # setup the corner edge detection ROI of left lips\n",
    "        left_lip_corner_x = (landmarks.left_lip_corner[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        left_lip_corner_y = (landmarks.left_lip_corner[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        \n",
    "        # facial landmarks corner axis\n",
    "        faceLandmarks.append([left_lip_corner_x, left_lip_corner_y])\n",
    "        # Left Eye box setup\n",
    "        leftEyeBox = self.createEyeBoundingBox(faceLandmarks[0], \n",
    "                                    faceLandmarks[1],\n",
    "                                    1.8)\n",
    "\n",
    "        # Right Eye box Setup\n",
    "        RightEyeBox = self.createEyeBoundingBox(faceLandmarks[2], \n",
    "                                    faceLandmarks[3],\n",
    "                                    1.8)\n",
    "        \n",
    "        # To crop image using the eye bounding boxes\n",
    "        # img[y:y+h, x:x+w]\n",
    "        # setup the outcome\n",
    "        leftEyeBox_img = org_frame[leftEyeBox[1] : leftEyeBox[1] + leftEyeBox[3], \n",
    "                             leftEyeBox[0] : leftEyeBox[0] + leftEyeBox[2]]\n",
    "\n",
    "        # processing the right eye box arrays and processing to the image\n",
    "        RightEyeBox_img = org_frame[RightEyeBox[1] : RightEyeBox[1] + RightEyeBox[3], \n",
    "                             RightEyeBox[0] : RightEyeBox[0] + RightEyeBox[2]]\n",
    "        # Outcome\n",
    "        return (RightEyeBox_img, leftEyeBox_img)\n",
    "\n",
    "    \n",
    "    def draw_final_result(self, frame, roi, headAngle, landmarks, gaze_vector):\n",
    "        \"\"\"\n",
    "        Draw the final output on frame including facial detection input, \n",
    "        face landmarks, head angles and gaze vector\n",
    "        \"\"\"\n",
    "\n",
    "        faceBoundingBoxWidth = roi[0].size[0]\n",
    "        faceBoundingBoxHeight = roi[0].size[1]\n",
    "\n",
    "        if self.fd_out:     \n",
    "            # Draw Face detection bounding Box\n",
    "            for i in range(len(roi)):\n",
    "                # Draw face ROI border\n",
    "                cv2.rectangle(frame,\n",
    "                            tuple(roi[i].position), tuple(roi[i].position + roi[i].size),\n",
    "                            (0, 0, 255), 4)\n",
    "\n",
    "        # Draw headPoseAxes\n",
    "        # Here head_position_x --> angle_y_fc  # Yaw\n",
    "        #      head_position_y --> angle_p_fc  # Pitch\n",
    "        #      head_position_z --> angle_r_fc  # Roll\n",
    "        yaw = headAngle.head_position_x\n",
    "        pitch = headAngle.head_position_y\n",
    "        roll = headAngle.head_position_z\n",
    "\n",
    "        sinY = sin(yaw * pi / 180.0)\n",
    "        sinP = sin(pitch * pi / 180.0)\n",
    "        sinR = sin(roll * pi / 180.0)\n",
    "\n",
    "        cosY = cos(yaw * pi / 180.0)\n",
    "        cosP = cos(pitch * pi / 180.0)\n",
    "        cosR = cos(roll * pi / 180.0)\n",
    "        \n",
    "        axisLength = 0.4 * faceBoundingBoxWidth\n",
    "        xCenter = int(roi[0].position[0] + faceBoundingBoxWidth / 2)\n",
    "        yCenter = int(roi[0].position[1] + faceBoundingBoxHeight / 2)\n",
    "\n",
    "        if self.hp_out:   \n",
    "            #center to right\n",
    "            cv2.line(frame, (xCenter, yCenter), \n",
    "                            (((xCenter) + int (axisLength * (cosR * cosY + sinY * sinP * sinR))),\n",
    "                            ((yCenter) + int (axisLength * cosP * sinR))),\n",
    "                            (0, 0, 255), thickness=2)\n",
    "            #center to top\n",
    "            cv2.line(frame, (xCenter, yCenter), \n",
    "                            (((xCenter) + int (axisLength * (cosR * sinY * sinP + cosY * sinR))),\n",
    "                            ((yCenter) - int (axisLength * cosP * cosR))),\n",
    "                            (0, 255, 0), thickness=2)\n",
    "            \n",
    "            #Center to forward\n",
    "            cv2.line(frame, (xCenter, yCenter), \n",
    "                            (((xCenter) + int (axisLength * sinY * cosP)),\n",
    "                            ((yCenter) + int (axisLength * sinP))),\n",
    "                            (255, 0, 0), thickness=2)\n",
    "        \n",
    "        # Draw landmark \n",
    "        keypoints = [landmarks.left_eye,\n",
    "                landmarks.right_eye,\n",
    "                landmarks.nose_tip,\n",
    "                landmarks.left_lip_corner,\n",
    "                landmarks.right_lip_corner]\n",
    "        \n",
    "        if self.lm_out:\n",
    "            for point in keypoints:\n",
    "                center = roi[0].position + roi[0].size * point\n",
    "                cv2.circle(frame, tuple(center.astype(int)), 2, (255, 255, 0), 4)\n",
    "            \n",
    "        # Draw Gaz vector with final frame\n",
    "        left_eye_x = (landmarks.left_eye[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        left_eye_y = (landmarks.left_eye[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        \n",
    "        right_eye_x = (landmarks.right_eye[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        right_eye_y = (landmarks.right_eye[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        \n",
    "        nose_tip_x = (landmarks.nose_tip[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        nose_tip_y = (landmarks.nose_tip[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        \n",
    "        left_lip_corner_x = (landmarks.left_lip_corner[0] * faceBoundingBoxWidth + roi[0].position[0])\n",
    "        left_lip_corner_y = (landmarks.left_lip_corner[1] * faceBoundingBoxHeight + roi[0].position[1])\n",
    "        \n",
    "        leftEyeMidpoint_start = int(((left_eye_x + right_eye_x)) / 2)\n",
    "        leftEyeMidpoint_end = int(((left_eye_y + right_eye_y)) / 2)\n",
    "        rightEyeMidpoint_start = int((nose_tip_x + left_lip_corner_x) / 2)\n",
    "        rightEyeMidpoint_End = int((nose_tip_y + left_lip_corner_y) / 2)\n",
    "        \n",
    "        # Gaze out\n",
    "        arrowLength = 0.4 * faceBoundingBoxWidth\n",
    "        gaze = gaze_vector[0]\n",
    "        gazeArrow_x = int((gaze[0]) * arrowLength)\n",
    "        gazeArrow_y = int(-(gaze[1]) * arrowLength)\n",
    "\n",
    "        if self.gm_out:\n",
    "            cv2.arrowedLine(frame, \n",
    "                            (leftEyeMidpoint_start, leftEyeMidpoint_end), \n",
    "                            ((leftEyeMidpoint_start + gazeArrow_x), \n",
    "                            leftEyeMidpoint_end + (gazeArrow_y)),\n",
    "                            (0, 255, 0), 3)\n",
    "\n",
    "            cv2.arrowedLine(frame, \n",
    "                            (rightEyeMidpoint_start, rightEyeMidpoint_End), \n",
    "                            ((rightEyeMidpoint_start + gazeArrow_x), \n",
    "                            rightEyeMidpoint_End + (gazeArrow_y)),\n",
    "                            (0, 255, 0), 3)\n",
    "        \n",
    "        \n",
    "        if self.print_perf_stats:\n",
    "            log.info('Performance stats:')\n",
    "            log.info(self.frame_processor.get_performance_stats())\n",
    "            \n",
    "    def get_mouse_point(self, headPosition, gaze_vector):\n",
    "        yaw = headPosition.head_position_x\n",
    "        pitch = headPosition.head_position_y\n",
    "        roll = headPosition.head_position_z\n",
    "        \n",
    "        sinR = sin(roll * pi / 180.0)\n",
    "        cosR = cos(roll * pi / 180.0)\n",
    "\n",
    "        gaze_vector = gaze_vector[0]\n",
    "        mouse_x = gaze_vector[0] * cosR + gaze_vector[1] * sinR\n",
    "        mouse_y =-gaze_vector[0] * sinR + gaze_vector[1] * cosR\n",
    "\n",
    "        return (mouse_x, mouse_y)\n",
    "\n",
    "    \n",
    "    def display_interactive_window(self, frame):\n",
    "        \"\"\"\n",
    "        Display using CV Window\n",
    "        \n",
    "        Args:\n",
    "            frame: \n",
    "                   The input frame\n",
    "        \"\"\"\n",
    "\n",
    "        color = (255, 255, 255)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text_scale = 0.5\n",
    "        text = (\"Press '%s' key to exit\" % (self.BREAK_KEY_LABELS))\n",
    "        thickness = 2\n",
    "        text_size = cv2.getTextSize(text, font, text_scale, thickness)\n",
    "        origin = np.array([frame.shape[-2] - text_size[0][0] - 10, 10])\n",
    "        line_height = np.array([0, text_size[0][1]]) * 1.5\n",
    "        cv2.putText(frame, text,\n",
    "                    tuple(origin.astype(int)), font, text_scale, color, thickness)\n",
    "\n",
    "        cv2.imshow('Visualization Window', frame)\n",
    "\n",
    "    \n",
    "    def should_stop_display(self):\n",
    "        \"\"\"\n",
    "        Check exit key from user\n",
    "        \"\"\"\n",
    "        key = cv2.waitKey(self.frame_timeout) & 0xFF\n",
    "        \n",
    "        return (key in self.BREAK_KEYS)\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(frame, crop_size):\n",
    "        \"\"\"\n",
    "        Center the image in the view\n",
    "        \"\"\"\n",
    "        fh, fw, fc = frame.shape\n",
    "        crop_size[0] = min(fw, crop_size[0])\n",
    "        crop_size[1] = min(fh, crop_size[1])\n",
    "        return frame[(fh - crop_size[1]) // 2 : (fh + crop_size[1]) // 2,\n",
    "                     (fw - crop_size[0]) // 2 : (fw + crop_size[0]) // 2,\n",
    "                     :]\n",
    "    \n",
    "    @staticmethod\n",
    "    def open_input_stream(path):\n",
    "        \"\"\"\n",
    "        Open the input stream\n",
    "        \"\"\"\n",
    "        log.info(\"Reading input data from '%s'\" % (path))\n",
    "        stream = path\n",
    "        try:\n",
    "            stream = int(path)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return (cv2.VideoCapture(stream))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def open_output_stream(path, fps, frame_size):\n",
    "        \"\"\"\n",
    "        Open the output stream\n",
    "        \"\"\"\n",
    "        output_stream = None\n",
    "        if path != \"\":\n",
    "            if not path.endswith('.avi'):\n",
    "                log.warning(\"Output file extension is not '.avi'. \" \\\n",
    "                        \"Some issues with output can occur, check logs.\")\n",
    "            log.info(\"Writing output to '%s'\" % (path))\n",
    "            output_stream = cv2.VideoWriter(path,\n",
    "                                            cv2.VideoWriter.fourcc(*'MJPG'), fps, frame_size)\n",
    "        return (output_stream)\n",
    "\n",
    "    def run(self, args):\n",
    "        \"\"\"\n",
    "        Driver function trigger all the function\n",
    "        Args:\n",
    "        args: \n",
    "                Input args\n",
    "        \"\"\"\n",
    "        # Open Input stream\n",
    "        # We camera node is 0\n",
    "        if args.input == \"cam\":\n",
    "            path = \"0\"\n",
    "        else:\n",
    "            path = args.input\n",
    "\n",
    "        input_stream = Mouse_Controller.open_input_stream(path)\n",
    "        \n",
    "        if input_stream is None or not input_stream.isOpened():\n",
    "            log.error(\"Cannot open input stream: %s\" % args.input)\n",
    "        \n",
    "        # FPS init\n",
    "        fps = input_stream.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        # Get the Frame org size\n",
    "        frame_size = (int(input_stream.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                      int(input_stream.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "        \n",
    "        # Get the frame count if its a video\n",
    "        self.frame_count = int(input_stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Crop the image if the user define input W, H\n",
    "        if args.crop_width and args.crop_height:\n",
    "            crop_size = (args.crop_width, args.crop_height)\n",
    "            #crop_size = (144, 240)\n",
    "            frame_size = tuple(np.minimum(frame_size, crop_size))\n",
    "\n",
    "        log.info(\"Input stream info: %d x %d @ %.2f FPS\" % \\\n",
    "            (frame_size[0], frame_size[1], fps))\n",
    "        \n",
    "        # Writer or CV Window\n",
    "        output_stream = Mouse_Controller.open_output_stream(args.output, fps, frame_size)\n",
    "        log.info(\"Input stream file opened\")\n",
    "\n",
    "        # Process on Input stream\n",
    "        self.process(input_stream, output_stream)\n",
    "\n",
    "        # Release Output stream if the writer selected\n",
    "        if output_stream:\n",
    "            output_stream.release()\n",
    "        \n",
    "        # Relese input stream[video or Camera node]\n",
    "        if input_stream:\n",
    "            input_stream.release()\n",
    "\n",
    "        # Distroy CV Window\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def process(self, input_stream, output_stream):\n",
    "        \"\"\"\n",
    "        Function to capture a frame from input stream, Pre-process,\n",
    "        Predict, and Display\n",
    "\n",
    "        Args:\n",
    "        input_stream: \n",
    "                The input file[Image, Video or Camera Node]\n",
    "        output_stream: \n",
    "                CV writer or CV window\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_stream = input_stream\n",
    "        self.output_stream = output_stream\n",
    "        frame_count = 0\n",
    "        # Loop input stream until frame is None\n",
    "        while input_stream.isOpened():\n",
    "            has_frame, frame = input_stream.read()\n",
    "            if not has_frame:\n",
    "                break\n",
    "            \n",
    "            frame_count+=1\n",
    "            #if self.input_crop is not None:\n",
    "                #frame = Mouse_Controller.center_crop(frame, self.input_crop)\n",
    "            \n",
    "            #self.org_frame = frame.copy()\n",
    "\n",
    "            # Get Face detection\n",
    "            detections = self.frame_processor.face_detector_process(frame)\n",
    "            \n",
    "            # Since other three models are depend on face detection. Continue\n",
    "            # only if detection happens\n",
    "            if not detections:\n",
    "                continue\n",
    "\n",
    "            # Get head Position\n",
    "            headPosition = self.frame_processor.head_position_estimator_process(frame)\n",
    "\n",
    "            # Get face landmarks \n",
    "            landmarks = self.frame_processor.face_landmark_detector_process(frame)\n",
    "\n",
    "            # Draw detection keypoints\n",
    "            output = self.landmarkPostProcessing(frame, landmarks[0], detections, self.org_frame)\n",
    "\n",
    "            gaze = self.frame_processor.gaze_estimation_process(headPosition, \n",
    "                                output[0], output[1])\n",
    "            gaze_vector = gaze[0]\n",
    "            \n",
    "            gaze_vector = gaze_vector['gaze_vector']\n",
    "\n",
    "            self.draw_final_result(frame, detections, headPosition, \n",
    "                                   landmarks[0], gaze_vector)\n",
    "            \n",
    "            if self.mc_out:\n",
    "                # This count can be removed if you have high performance system\n",
    "                if frame_count % 10 == 0:\n",
    "                    mouse_x, mouse_y = self.get_mouse_point(headPosition, gaze_vector)\n",
    "                    \n",
    "                    self.mc.move(mouse_x, mouse_y)\n",
    "\n",
    "            # Write on disk \n",
    "            if output_stream:\n",
    "                output_stream.write(frame)\n",
    "            \n",
    "            # Display on CV Window\n",
    "            if self.display:\n",
    "                self.display_interactive_window(frame)\n",
    "                if self.should_stop_display():\n",
    "                    break\n",
    "            \n",
    "            # Update FPS\n",
    "            FPS = self.update_fps()\n",
    "            print(\"[INFO] approx. FPS: {:.2f}\".format(FPS))\n",
    "            self.frame_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ie_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ie_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ie_module.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "######################################################################################################\n",
    "#\n",
    "#                                     IE Module Script\n",
    "#\n",
    "######################################################################################################\n",
    "\n",
    "'''\n",
    "OpenVINO IE Module Script: \n",
    "\n",
    "This script represents the inference classe of the plugin devices and based on the hardware selection the \n",
    "operation will be executed with the help of the OpenVINO libraries.\n",
    "\n",
    "Device = {CPU, GPU, FPGU, MYRID, HDDL, VPU}\n",
    "\n",
    "[Doc](https://docs.openvinotoolkit.org/2018_R5/_docs_IE_DG_inference_engine_intro.html)\n",
    "'''\n",
    "# Load the library\n",
    "import logging as log\n",
    "import os.path as osp\n",
    "\n",
    "# load OpenVINO library\n",
    "from openvino.inference_engine import IEPlugin\n",
    "\n",
    "class Inference_Context:\n",
    "    '''\n",
    "    Inference Enginer API \n",
    "\n",
    "    Loading the Plugins for the hardware inputs. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initialize the plugins\n",
    "        self.plugins = {}\n",
    "\n",
    "    def load_plugins(self, devices, cpu_ext=\"\", gpu_ext=\"\"):\n",
    "        \n",
    "        # Load the plugin device as CPU or GPU instruction\n",
    "        log.info(\"Loading plugins for devices: %s\" % (devices))\n",
    "\n",
    "        plugins = { d: IEPlugin(d) for d in devices }\n",
    "        \n",
    "        # if the plugin selected then the device fetech the instruction path\n",
    "        # for CPU Hardware\n",
    "        if 'CPU' in plugins and not len(cpu_ext) == 0:\n",
    "            log.info(\"Using CPU extensions library '%s'\" % (cpu_ext))\n",
    "            assert osp.isfile(cpu_ext), \"Failed to open CPU extensions library\"\n",
    "            plugins['CPU'].add_cpu_extension(cpu_ext)\n",
    "        \n",
    "        # for GPU Hardware\n",
    "        if 'GPU' in plugins and not len(gpu_ext) == 0:\n",
    "            assert osp.isfile(gpu_ext), \"Failed to open GPU definitions file\"\n",
    "            plugins['GPU'].set_config({\"CONFIG_FILE\": gpu_ext})\n",
    "        \n",
    "        # set the plugins\n",
    "        self.plugins = plugins\n",
    "        \n",
    "        # generating the log information of the plugins loaded\n",
    "        log.info(\"Plugins are loaded\")\n",
    "\n",
    "    def get_plugin(self, device):\n",
    "        \n",
    "        # getting the plugin device\n",
    "        return self.plugins.get(device, None)\n",
    "\n",
    "    def check_model_support(self, net, device):\n",
    "        \n",
    "        '''\n",
    "        Check the model support\n",
    "        This is important steps for the device to process the inference API pipeline\n",
    "        '''\n",
    "        \n",
    "        plugin = self.plugins[device]\n",
    "\n",
    "        \n",
    "        if plugin.device == \"CPU\":\n",
    "            supported_layers = plugin.get_supported_layers(net)\n",
    "            not_supported_layers = [l for l in net.layers.keys() \\\n",
    "                                    if l not in supported_layers]\n",
    "\n",
    "            if len(not_supported_layers) != 0:\n",
    "                log.error(\"The following layers are not supported \" \\\n",
    "                    \"by the plugin for the specified device {}:\\n {}\" \\\n",
    "                    .format(plugin.device, ', '.join(not_supported_layers)))\n",
    "                log.error(\"Please try to specify cpu extensions \" \\\n",
    "                    \"library path in the command line parameters using \" \\\n",
    "                    \"the '-l' parameter\")\n",
    "\n",
    "                raise NotImplementedError(\n",
    "                    \"Some layers are not supported on the device\")\n",
    "\n",
    "    def load_model(self, model, device, max_requests=1):\n",
    "        \n",
    "        # Load the model with the device\n",
    "        self.check_model_support(model, device)\n",
    "        plugin = self.plugins[device]\n",
    "        \n",
    "        # plugin process for the API request\n",
    "        deployed_model = plugin.load(network=model, num_requests=max_requests)\n",
    "        return (deployed_model)\n",
    "\n",
    "class Module(object):\n",
    "    '''\n",
    "    Module Class: \n",
    "    This class represent the technical aspects of the modules for processing the data and model. \n",
    "    the request which are active and process get the output and performance results. \n",
    "\n",
    "    It provide performance and deployment results.\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.model = model\n",
    "        self.device_model = None\n",
    "        \n",
    "        # setting the request to 0 for inital stage\n",
    "        self.max_requests = 0\n",
    "        self.active_requests = 0\n",
    "        \n",
    "        # clearing the points\n",
    "        self.clear()\n",
    "\n",
    "    def deploy(self, device, context, queue_size=1):\n",
    "        \n",
    "        # deploy the context and the device with 1 queue process\n",
    "        self.context = context\n",
    "        self.max_requests = queue_size\n",
    "        \n",
    "        # load the model with the device input instruction\n",
    "        self.device_model = context.load_model(\n",
    "            self.model, device, self.max_requests)\n",
    "        self.model = None\n",
    "\n",
    "    def enqueue(self, input):\n",
    "        \n",
    "        self.clear()\n",
    "        \n",
    "        # clear the results\n",
    "        if self.max_requests <= self.active_requests:\n",
    "            log.warning(\"Processing request rejected - too many requests\")\n",
    "            return False\n",
    "        \n",
    "        # device model async operation\n",
    "        self.device_model.start_async(self.active_requests, input)\n",
    "        self.active_requests += 1\n",
    "        return (True)\n",
    "\n",
    "    def wait(self):\n",
    "        \n",
    "        # wait list\n",
    "        if self.active_requests <= 0:\n",
    "            return ()\n",
    "\n",
    "        self.perf_stats = [None, ] * self.active_requests\n",
    "        self.outputs = [None, ] * self.active_requests\n",
    "        for i in range(self.active_requests):\n",
    "            self.device_model.requests[i].wait()\n",
    "            self.outputs[i] = self.device_model.requests[i].outputs\n",
    "            self.perf_stats[i] = self.device_model.requests[i].get_perf_counts()\n",
    "        \n",
    "        # if the wait request are process finishes\n",
    "        self.active_requests = 0\n",
    "\n",
    "    def get_outputs(self):\n",
    "        \n",
    "        # output results from the wait request complete cycle for operation\n",
    "        self.wait()\n",
    "        return (self.outputs)\n",
    "\n",
    "    def get_performance_stats(self):\n",
    "        \n",
    "        # Getting the performace statiscs results\n",
    "        return (self.perf_stats)\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        \n",
    "        # Clearing the stats and output results\n",
    "        self.perf_stats = []\n",
    "        self.outputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting helper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile helper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#########################################################################################################\n",
    "#\n",
    "#                                        Helper Script\n",
    "#\n",
    "########################################################################################################\n",
    "'''\n",
    "Helper Script: \n",
    "\n",
    "This script is processing technique for the ROI region to map in the frame. The frame are process numerically and based on the taget shape the dimension are set. \n",
    "The default batch is set as 1 and the frame are pre-processed for better outcome.\n",
    "'''\n",
    "# load libary\n",
    "import cv2 # Load OpenCV library\n",
    "import numpy as np # load numerical operation library\n",
    "from numpy import clip # load clip library\n",
    "\n",
    "def cut_roi(frame, roi):\n",
    "    # set the position of the ROI region\n",
    "    p1 = roi.position.astype(int)\n",
    "    p1 = clip(p1, [0, 0], [frame.shape[-1], frame.shape[-2]])\n",
    "    p2 = (roi.position + roi.size).astype(int)\n",
    "    p2 = clip(p2, [0, 0], [frame.shape[-1], frame.shape[-2]])\n",
    "    # frame array pointers\n",
    "    return (np.array(frame[:, :, p1[1]:p2[1], p1[0]:p2[0]]))\n",
    "\n",
    "def cut_rois(frame, rois):\n",
    "    # cut the ROI in the frame\n",
    "    return [cut_roi(frame, roi) for roi in rois]\n",
    "\n",
    "def resize_input(frame, target_shape):\n",
    "    # the frame are outline with the target shape. \n",
    "    # Target shape dimensions\n",
    "    assert len(frame.shape) == len(target_shape), \\\n",
    "        \"Expected a frame with %s dimensions, but got %s\" % \\\n",
    "        (len(target_shape), len(frame.shape))\n",
    "\n",
    "    assert frame.shape[0] == 1, \"Only batch size 1 is supported\"\n",
    "    n, c, h, w = target_shape\n",
    "\n",
    "    input = frame[0]\n",
    "    # process the target shape income and the transpose it.\n",
    "    if not np.array_equal(target_shape[-2:], frame.shape[-2:]):\n",
    "        input = input.transpose((1, 2, 0)) # process to HWC\n",
    "        input = cv2.resize(input, (w, h))\n",
    "        input = input.transpose((2, 0, 1)) # process to CHW\n",
    "\n",
    "    return (input.reshape((n, c, h, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#############################################################################################################\n",
    "#\n",
    "#                                 Computer Pointer Controller Main Script\n",
    "#\n",
    "#############################################################################################################\n",
    "\n",
    "'''\n",
    "Computer Pointer Controller:\n",
    "This is the main script for the running all the code and the functions. The computer pointer takes the input parameter.\n",
    "Input Arguments:\n",
    "        1. Model-01 -> Face Detection Model\n",
    "        2. Model-02 -> Head Pose Estimation Model \n",
    "        3. Model-03 -> Landmark Detection Model \n",
    "        4. Model-04 -> Gaze Estimator Model\n",
    "        5. Input (Video or Webcam) -> media file in .mp4 or CAM\n",
    "        6. Device -> 'CPU', 'GPU', 'FPGA', 'MYRIAD', 'HETERO', 'HDDL'\n",
    "        7. Flags -> Show if enabled option for models \n",
    "        8. Resolution -> Width and Height (Optional)\n",
    "\n",
    "Output Arguments: \n",
    "        1. Media -> Output on screen and save file\n",
    "        2. timelapse -> video time inference in fps with seconds\n",
    "        3. Samples -> Output of the model outcome as per the Flags initiated\n",
    "        4. perf_stats -> Statics of the inference backend\n",
    "'''\n",
    "\n",
    "# load the library\n",
    "# load the system libary\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# load numerical operation library\n",
    "import numpy as np\n",
    "from math import cos, sin, pi \n",
    "\n",
    "# load the log librarys\n",
    "import logging as log\n",
    "\n",
    "# load OpenCV library\n",
    "import cv2\n",
    "\n",
    "# load the Argument Parser for user input\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# load the model and input feeder library (custom)\n",
    "from ie_module import Inference_Context\n",
    "from helper import cut_rois, resize_input\n",
    "from face_detection import Face_Detection\n",
    "from head_postion_estimation import Head_Pose_Estimator\n",
    "from landmark_detection import Landmarks_Detection\n",
    "from gaze_Estimator import Gaze_Estimation\n",
    "#from mouse_controller import Mouse_Controller_Pointer\n",
    "#from mouse_process import Mouse_Controller\n",
    "\n",
    "# load the OpenVINO library\n",
    "from openvino.inference_engine import IENetwork\n",
    "\n",
    "\n",
    "# Set the Device operation types\n",
    "DEVICE_KINDS = ['CPU', 'GPU', 'FPGA', 'MYRIAD', 'HETERO', 'HDDL']\n",
    "\n",
    "def build_argparser():\n",
    "    \"\"\"\n",
    "    Parse command line arguments.\n",
    "    -i bin/demo.mp4 \n",
    "    -m_fd <path>models/intel/face-detection-adas-binary-0001/FP32-INT1/face-detection-adas-binary-0001.xml \n",
    "    -d_fd { 'CPU', 'GPU', 'FPGA', 'MYRIAD', 'HETERO', 'HDDL'}\n",
    "    -o_fd \n",
    "    -m_hp <path>models/intel/head-pose-estimation-adas-0001/FP16/head-pose-estimation-adas-0001.xml \n",
    "    -d_hp {'CPU', 'GPU', 'FPGA', 'MYRIAD', 'HETERO', 'HDDL'}\n",
    "    -o_hp \n",
    "    -m_lm <path>mo_model/intel/landmarks-regression-retail-0009/FP16/landmarks-regression-retail-0009.xml \n",
    "    -d_lm {'CPU', 'GPU', 'FPGA', 'MYRIAD', 'HETERO', 'HDDL'}\n",
    "    -o_lm \n",
    "    -m_gm <path>mo_model/intel/gaze-estimation-adas-0002/FP16/gaze-estimation-adas-0002.xml \n",
    "    -d_gm {'CPU', 'GPU', 'FPGA', 'MYRIAD', 'HETERO', 'HDDL'}\n",
    "    -o_gm \n",
    "    -o <path>results/outcome<num>\n",
    "    -pc \n",
    "\n",
    "    :return: command line arguments\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"-i\", \"--input\", required=True, type=str,\n",
    "                        help=\"Path to image or video file in .mp4 format or enter CAM for webcam\")\n",
    "    \n",
    "  \n",
    "    parser.add_argument(\"-m_fd\", \"--model_face_detection\", required=True, type=str,\n",
    "                        help=\"Path to load an .xml file with a trained Face Detection model\")               \n",
    "    \n",
    "    parser.add_argument('-d_fd', default='CPU', choices=DEVICE_KINDS,\n",
    "                       help=\"(optional) Target device for the \" \\\n",
    "                       \"Face Detection model device selection (default: %(default)s)\")\n",
    "\n",
    "    parser.add_argument('-t_fd', metavar='[0..1]', type=float, default=0.4,\n",
    "                       help=\"(optional) Set the Probability threshold for face detections\" \\\n",
    "                       \"(default: %(default)s)\")\n",
    "\n",
    "    parser.add_argument('-o_fd', action='store_true',\n",
    "                       help=\"(optional) Process the face detection output\")\n",
    "                       \n",
    "    parser.add_argument(\"-m_hp\", \"--model_head_position\", required=True, type=str,\n",
    "                        help=\"Path to load an .xml file with a trained Head Pose Estimation model\")\n",
    "\n",
    "    parser.add_argument('-d_hp', default='CPU', choices=DEVICE_KINDS,\n",
    "                       help=\"(optional) Target device for the \" \\\n",
    "                       \"Head Position model (default: %(default)s)\")\n",
    "\n",
    "    parser.add_argument('-o_hp', action='store_true',\n",
    "                       help=\"(optional) Show Head Position output\")\n",
    "\n",
    "    parser.add_argument(\"-m_lm\", \"--model_landmark_regressor\", required=True, type=str,\n",
    "                        help=\"Path to load an .xml file with a trained Head Pose Estimation model\") \n",
    "    parser.add_argument('-d_lm', default='CPU', choices=DEVICE_KINDS,\n",
    "                       help=\"(optional) Target device for the \" \\\n",
    "                       \"Facial Landmarks Regression model (default: %(default)s)\")\n",
    "\n",
    "    parser.add_argument('-o_lm', action='store_true',\n",
    "                       help=\"(optional) Show Landmark detection output\")\n",
    "    \n",
    "    parser.add_argument(\"-m_gm\", \"--model_gaze\", required=True, type=str,\n",
    "                        help=\"Path to an .xml file with a trained Gaze Estimation model\")\n",
    "\n",
    "    parser.add_argument('-d_gm', default='CPU', choices=DEVICE_KINDS,\n",
    "                       help=\"(optional) Target device for the \" \\\n",
    "                       \"Gaze estimation model (default: %(default)s)\")\n",
    "\n",
    "    parser.add_argument('-o_gm', action='store_true',\n",
    "                       help=\"(optional) Show Gaze estimation output\")\n",
    "    \n",
    "    parser.add_argument('-o_mc', action='store_true',\n",
    "                       help=\"(optional) Run mouse counter\")\n",
    "\n",
    "    parser.add_argument('-pc', '--perf_stats', action='store_true',\n",
    "                       help=\"(optional) Output detailed per-layer performance stats\")\n",
    "\n",
    "    parser.add_argument('-exp_r_fd', metavar='NUMBER', type=float, default=1.20,\n",
    "                       help=\"(optional) Scaling ratio for bboxes passed to face recognition \" \\\n",
    "                       \"(default: %(default)s)\")\n",
    "\n",
    "    parser.add_argument('-cw', '--crop_width', default=0, type=int,\n",
    "                        help=\"(optional) Crop the input stream to this width \" \\\n",
    "                        \"(default: no crop). Both -cw and -ch parameters \" \\\n",
    "                        \"should be specified to use crop.\")\n",
    "\n",
    "    parser.add_argument('-ch', '--crop_height', default=0, type=int,\n",
    "                        help=\"(optional) Crop the input stream to this width \" \\\n",
    "                        \"(default: no crop). Both -cw and -ch parameters \" \\\n",
    "                        \"should be specified to use crop.\")\n",
    "                        \n",
    "    parser.add_argument('-v', '--verbose', action='store_true',\n",
    "                       help=\"(optional) Be more verbose\")\n",
    "\n",
    "    parser.add_argument('-l', '--cpu_lib', metavar=\"PATH\", default=\"\",\n",
    "                       help=\"(optional) For MKLDNN (CPU)-targeted custom layers, if any. \" \\\n",
    "                       \"Path to a shared library with custom layers implementations\")\n",
    "\n",
    "    parser.add_argument('-c', '--gpu_lib', metavar=\"PATH\", default=\"\",\n",
    "                       help=\"(optional) For clDNN (GPU)-targeted custom layers, if any. \" \\\n",
    "                       \"Path to the XML file with descriptions of the kernels\")\n",
    "\n",
    "    parser.add_argument('-tl', '--timelapse', action='store_true',\n",
    "                         help=\"(optional) Auto-pause after each frame\")\n",
    "\n",
    "    parser.add_argument('-o', '--output', metavar=\"PATH\", default=\"\",\n",
    "                         help=\"(optional) Path to save the output video to\")\n",
    "\n",
    "\n",
    "    return (parser)\n",
    "\n",
    "        \n",
    "##########################################################################################################\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args = build_argparser().parse_args()\n",
    "    \n",
    "    log.basicConfig(format=\"[ %(levelname)s ] %(asctime)-15s %(message)s\",\n",
    "                    level=log.INFO if not args.verbose else log.DEBUG, stream=sys.stdout)\n",
    "\n",
    "    #driverMonitoring = Mouse_Controller(args)\n",
    "    #driverMonitoring.run(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step \n",
    "\n",
    "Go to the next notebook module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
